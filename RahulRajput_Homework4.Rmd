---
title: "Homework4_RahulRajput"
author: "Rahul Rajput"
date: '`r Sys.Date()`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}

#### ANSWER 1 ####

autoratings = read.csv("Automobile Service Center Ratings.csv")
#autoratings

#Part A

# Objective: Determine if people who say they will return give higher ratings for each category 
# Type of Data: Ordinal
# Types of Samples: Independent Block Design
# Definition: Population 1 is people who say they will return, Population 2 is people who say they will not return
# Test to be used: Non Parametric, Friedman Test (rank within each block, sum ranks over columns, chi-squared if sample size >5)

# Stack and factor by customerid and categories

# H0: The two population locations are the same
# H1: Population 1 lies to the right of Population 2

autoratings_parta = autoratings[c('Quality','Fairness','Guarantee','Checkout','Return')]
autoratings_parta$CustomerID = seq.int(nrow(autoratings_parta))

library('reshape2')
#install.packages('stats')
library('stats')

autoratings_parta_stacked = melt(autoratings_parta, id.vars = 'CustomerID')
#autoratings_parta_stacked

names(autoratings_parta_stacked) = c('CustoemrID','Categories','Ratings')

ratings = autoratings_parta_stacked$Ratings
customerid = factor(autoratings_parta_stacked$CustoemrID)
categories = factor(autoratings_parta_stacked$Categories)

friedman.test(ratings,categories,customerid)
friedman.test(as.matrix(autoratings_parta))

sprintf("Since the p-value is extremely low we reject the Null Hypothesis. There appears to be sufficient evidence that People who say they will return rate the firm higher than People who say they will not return.")

# Part B

# Objective: Determine if people who say they will return give higher ratings for each category 
# Type of Data: Ordinal
# Types of Samples: Independent Block Design
# Definition: Populations - People who leave positive, negative, and no comments
# Test to be used: Non Parametric, Friedman Test (rank within each block, sum ranks over columns, chi-squared if sample size >5)

# Stack and factor by customerid and categories

# H0: The three population locations are the same
# H1: Atleast one population is different

autoratings_partb = autoratings[c('Quality','Fairness','Guarantee','Checkout','Comment')]
autoratings_partb$CustomerID = seq.int(nrow(autoratings_partb))
autoratings_partb_stacked = melt(autoratings_partb, id.vars = 'CustomerID')
#autoratings_partb_stacked
names(autoratings_partb_stacked) = c('CustomerID','Categories','Ratings')

ratingsb = autoratings_partb_stacked$Ratings
customeridb = factor(autoratings_partb_stacked$CustomerID)
categoriesb = factor(autoratings_partb_stacked$Categories)

friedman.test(ratingsb,categoriesb,customeridb)
friedman.test(as.matrix(autoratings_partb))

sprintf("Since the p-value is extremely low we reject the Null Hypothesis. There appears to be sufficient evidence that People who leave positive, negative, or no comments differ in the ratings provided by them.")
```


```{r}

#### ANSWER 2 ####

softdrinks = read.csv("Soft Drink Recipe.csv")
#softdrinks

# Objective: Determine if people rate the three recipes differently 
# Type of Data: Ordinal
# Types of Samples: Independent Block Design
# Definition: Ratings of the 3 recipes
# Test to be used: Non Parametric, Friedman Test (rank within each block, sum ranks over columns, chi-squared if sample size >5)
# 5% significance level

softdrinks_stacked = melt(softdrinks, id.vars = 'Person')
names(softdrinks_stacked) = c('Person','Recipe','rating2')

person2 = softdrinks_stacked$Person
recipes = factor(softdrinks_stacked$Recipe)
ratings2 = factor(softdrinks_stacked$rating2)

#H0: The ratings for the 3 populations (recipes) are the same
#H1: Atleast one population is different

friedman.test(ratings2,recipes,person2)
friedman.test(as.matrix(softdrinks[c('Original','New.Recipe.1','New.Recipe..2')]))

sprintf("Since the p-value is below 0.05 we reject the Null Hypothesis. We can infer that there is a difference in the ratings provided to each recipe.")

```

```{r}

#### ANSWER 3 ####

jobloss = read.csv("Job Loss.csv")
jobloss_new = jobloss[c("CASEID","HRS1","JOBLOSE")]
#jobloss_new

library('dplyr')
jobloss_new1 = jobloss %>%
  filter(jobloss_new$JOBLOSE==1)
hist(jobloss_new1$HRS1)
jobloss_new2 = jobloss %>%
  filter(jobloss_new$JOBLOSE==2)
hist(jobloss_new2$HRS1)
jobloss_new3 = jobloss %>%
  filter(jobloss_new$JOBLOSE==3)
hist(jobloss_new3$HRS1)
jobloss_new4 = jobloss %>%
  filter(jobloss_new$JOBLOSE==4)
hist(jobloss_new4$HRS1)

# HRS1 data is interval, checking for normality by histogram
hist(jobloss_new$HRS1)
# Data is not normal

# Objective: Determine if working more decreases probability of losing job
# Type of Data: Interval, Non Normal
# Types of Samples: Independent samples, more than 2 populations
# Definition: 4 Populations
# Test to be used: Non Parametric, Kruskal-Wallis
# 5% significance level

#H0: Location of all 4 populations (probability of losing job) is same
#H1: Location of atleast two populations is different

kruskal.test(jobloss_new$HRS1~factor(jobloss_new$JOBLOSE))

sprintf("Since the p-value is lower than 0.05 we can reject the Null Hypothesis at the 5%% significance level.")

#Running post-hoc test: Pairwise Wilcoxon Test for multiple comparison
pairwise.wilcox.test(jobloss_new$HRS1, jobloss_new$JOBLOSE, p.adjust = 'bonferroni')

#install.packages("PMCMRplus")
library("PMCMRplus")

kwAllPairsDunnTest(x=jobloss_new$HRS1, g=jobloss_new$JOBLOSE, p.adjust.method = "bonferroni")

#Try Spearman
# Objective: Determine if working more decreases probability of losing job
# Type of Data: Interval Non Normal and Ordinal
# Types of Samples: Independent samples
# Definition: Ratings of the 3 recipes
# Test to be used: Non Parametric, Kruskal-Wallis
# 5% significance level

# H0: rho = 0
# H1: rho > 0

cor.test(x = jobloss_new$HRS1, y = jobloss_new$JOBLOSE, 
         alternative = "greater",
         method = "spearman", 
         exact = FALSE)

sprintf("Since the p-value is significant we reject the Null Hypothesis. We can infer that more hours the poeple work the less likely they are to be fired")
```

```{r}

#### ANSWER 4 ####

icecream = read.csv("Ice Cream Comparison.csv")
#icecream
icecream_stacked = stack(icecream)
names(icecream_stacked) = c("Ratings","Brand")
#icecream_stacked
ratings4 = as.integer(icecream_stacked$Ratings)
brands4 = as.factor(icecream_stacked$Brand)

# Objective: Determine if people rate the ice cream brands differently
# Type of Data: Ordinal
# Types of Samples: Dependent, 2 samples
# Definition: Compare average ratings given to European and Domestic brands
# Test to be used: Non Parametric, Sign, 10% significance level
# In this case we calculate difference b/w sample 1 observation and sample 2 observation for each pair and take difference of sign, the number of signs should more or less be evenly distributed.
# The null hypothesis is that the proportion of positive signs is 0.5, therefore the test is basically a z-test for proportion

# H0: Location of both populations (European and Domestic) are the same
# H1: Location of European Population is to the right of Domestic

#install.packages("PASWR2")
library("PASWR2")

SIGN.test(x = icecream$European, y = icecream$Domestic, alternative="greater", conf.level = 0.90)

sprintf("Since the p-value is highly significant we can conclude at the 10%% significant level that the European brand was rated higher.")

```

```{r}

#### ANSWER 5 ####

locksmith = read.csv("Machine Selection.csv")
#locksmith

# Objective: Compare cutting times for 2 machine
# Type of Data: Interval, Non Normal (Data is not normal)
# Types of Samples: Independent Block Design, 2 samples
# Definition: Compare cutting times for 2 machines
# Test to be used: Non Parametric, Friedman Test

hist(locksmith$Machine.1)
hist(locksmith$Machine.2)

library("reshape2")
locksmith_stacked = melt(locksmith, id.vars = 'Key')
names(locksmith_stacked) = c('Key','Machine','Time')

cuttingtime = locksmith_stacked$Time
key5 = factor(locksmith_stacked$Key)
machine = factor(locksmith_stacked$Machine)

# H0: The location of both populations (Machine cutting times) is the same
# H1: The locations are different

library("stats")
friedman.test(cuttingtime,machine,key5)

locksmith_trial = locksmith[c('Machine.1','Machine.2')]
friedman.test(data.matrix(locksmith_trial))

sprintf("We can reject the Null Hypothesis at the 10%% significance level. Therefore we can infer that there is a difference between cutting times for the two machines. The locksmith should go for the faster machine.")

```

```{r}

#### ANSWER 6 ####

creditcard = read.csv("CreditCardHolders.csv")
#creditcard
hist(creditcard$Applied)
hist(creditcard$Contacted)

shapiro.test(creditcard$Applied)
shapiro.test(creditcard$Contacted)

# Based on shapiro test we can infer that the data for Contacted customers is normally distributed but not so for Applied customers.

# Objective: Compare purchase amounts for 2 different types of customers
# Type of Data: Interval, Non Normal (Data is not normal)
# Types of Samples: Independent 2 samples
# Definition: Compare population locations for Applied and Contacted
# Test to be used: Non Parametric, Wilcoxson Rank Sum Test
# 5% significant

creditcard_stacked = stack(creditcard)
names(creditcard_stacked) = c("Purchases","CustType")
purchases = creditcard_stacked$Purchases
custtype = as.factor(creditcard_stacked$CustType)

# H0: Locations for both populations(Applied and Contacted customers) are the same
# H1: Populations locations are different

wilcox.test(purchases~custtype, alt="two.sided",paired=FALSE,exact=FALSE, conf.level = 0.95)
wilcox.test(creditcard_stacked$Purchases~creditcard_stacked$CustType, alt="two.sided",paired=FALSE,exact=FALSE, conf.level = 0.95)

sprintf("Since the p-value is greater than 0.05 we fail to reject the null hypothesis. There is no significant difference between the two ttypes of customers in terms of purchases.")
```

```{r}

#### ANSWER 7 ####

debt = read.csv("AmericanDebt.csv")
#debt

hist(debt$This.Year-debt$Last.Year)
shapiro.test(debt$This.Year-debt$Last.Year) # Data is Normal, re run

# Objective: Compare debt payments to income ratio over 2 years
# Type of Data: Interval, Normal
# Types of Samples: Independent 2 samples
# Definition: Compare population locations for ratios between last year and this year, Population 1 is This Year, Population 2 is Last Year
# Test to be used: Parametric, 2-Sample t-test
# 5% significance

# H0: Population 1 = Population 2
# H1: Population 1 > Population 2

t.test(debt$This.Year, debt$Last.Year, alternative = "greater")

sprintf("Since the p-value is large we fail to reject the Null Hypothesis. We can infer that there is no difference in the ratio of debt payments to household income between this year and the last.")

```

```{r}

#### ANSWER 8 ####

benefits = read.csv("Benefits Comparison.csv")
#benefits # Data is nominal, 2-sample prop test

# Part A
sprintf("Potential confounding effects include offering the different incentives to samples who live on different coasts. There might be potential confounding factors such as lifestyle differences, differences in quality of existing healthcare etc which may influence employees on different coasts to weight the incentives differently.")

# Part B

# Objective: Compare whether retetntion rates differ for the 2 incentives offered
# Type of Data: Nominal
# Types of Samples: Independent 2 samples 
# Test to be used: Parametric, 2-sample prop test
# 5% significance

# H0: The location of both populations (retention rates for incentives) is the same
# H1: Population 1 lies to the right of Population 2


health = benefits %>%
  filter(benefits$Benefit == "Health")

vacation = benefits %>%
  filter(benefits$Benefit == "Vacation")

healthretain = length(which(health$Retention==1))
healthleave = length(which(health$Retention==0))

vacationretain = length(which(vacation$Retention==1))
vacationleave = length(which(vacation$Retention==0))

numretain = c(healthretain, vacationretain)
numleave = c(healthleave, vacationleave)

numretain
numleave


prop.test(numleave,numretain, alternative = "greater", correct = FALSE)

sprintf("Since the p-value is greter than 0.05 we fail to reject the Null Hypothesis. Switching to providing health benefits does not have a significantly higher retention rate at the 95%% confidence level.")

# Part C

# H0: The location of both populations (retention rates for incentives) is the same
# H1: The location of populations is different

prop.test(numleave,numretain, alternative = "two.sided", correct = FALSE)

sprintf("There is statistically significant difference in the retention rates between the two strategies.")
```


```{r}

#### ANSWER 9 ####

wage = read.csv("Wage.csv")

wages = wage$Wage
educ = wage$Educ
exper = wage$Exper

# Running OLS model to calculate residuals

model_q9 = lm(wages ~ educ + exper)
summary(model_q9)

resids_q9 = residuals(model_q9)
absresids_q9 = abs(resids_q9)

# H0: Error term is Homoscedastic
# H1: Error term is Heteroscedastic

cor.test(x = educ, y = absresids_q9, 
         alternative = "greater",
         method = "spearman", 
         exact = FALSE)

cor.test(x = exper, y = absresids_q9, 
         alternative = "greater",
         method = "spearman", 
         exact = FALSE)

sprintf("Since the p-value is lower than 0.05 for both tests we can infer that there is evidence of heteroscedasticity in the model.")
```

```{r}

#### ANSWER 10 ####

comp = read.csv("Compensation.csv")
#comp

AvgComp = c(3396,3787,4013,4014,4146,4241,4387,4538,4843) #Y
AvgProd = c(9355,8584,7962,8275,8389,9418,9795,10281,11750) #X

# Part A
model_q10 = lm(AvgComp~AvgProd)
resids_q10 = residuals(model_q10)

# Part B - Conducting Park's Test
resids_q10_sq = resids_q10^2

### H0: B2 = 0 (No heteroscedasticity)
### H1: B2 =/= 0 (there is heteroscedasticity)

model_q10_park = lm(log(resids_q10_sq) ~ log(AvgProd))
summary(model_q10_park)

sprintf("Since the beta coefficient for log(X) is not significant, we fail to reject the Null Hypothesis. There is no evidene of Heteroscedasticity")

# Part C - Glejser Test

### H0: B2 = 0 (No heteroscedasticity)
### H1: B2 =/= 0 (there is heteroscedasticity)

model_q10_glejser1 = lm(abs(resids_q10) ~ AvgProd)
summary(model_q10_glejser1)

model_q10_glejser2 = lm(abs(resids_q10) ~ sqrt(AvgProd))
summary(model_q10_glejser2)

sprintf("The beta coefficient of X variable is not signifanct in either case therefore we fail to reject the Null Hypothesis. There is no evidence of Heteroscedasticity.")

# Part D - Spearman Rank Corr Test

# H0: Error term is Homoscedastic
# H1: Error term is Heteroscedastic 

cor.test(x = AvgProd, y = abs(resids_q10), 
         alternative = "greater",
         method = "spearman", 
         exact = FALSE)

sprintf("Since the p-value is not significant we can infer that there is evidence of heteroscedasticity in the model.")

```

```{r}

#### ANSWER 11 ####

rnd = read.csv("R&D.csv")
#rnd

RD = rnd$RD
SALES = rnd$SALES

model_q11 = lm(RD ~ SALES)
resids_q11 = residuals(model_q11)

# Park Test (Regressing ln(u^2) on ln(X))

### H0: B2 = 0 (No heteroscedasticity)
### H1: B2 =/= 0 (there is heteroscedasticity)

q11_park = lm(log(resids_q11^2) ~ log(SALES))
summary(q11_park)

sprintf("Since the beta coefficient for log(X) is not significant, we fail to reject the Null Hypothesis. There is no evidene of Heteroscedasticity")

# Glejser Test (Regressing |u| on X, sqrt X, 1/X)

### H0: B2 = 0 (No heteroscedasticity)
### H1: B2 =/= 0 (there is heteroscedasticity)

q11_glejser1 = lm(abs(resids_q11) ~ SALES)
summary(q11_glejser1)

sprintf("Since the p-value is less than 0.10 we reject the Null Hypothesis. There is evidence of Hetereoscedasticity at a 10%% significance level.")

q11_glejser2 = lm(abs(resids_q11) ~ sqrt(SALES))
summary(q11_glejser2)

sprintf("Since the p-value is less than 0.05 we reject the Null Hypothesis. There is evidence of Hetereoscedasticity at a 5%% significance level.")

# Spearman Rank Corr Tes (Corr between |u| and X)

# H0: Error term is Homoscedastic
# H1: Error term is Heteroscedastic 

cor.test(x = SALES, y = abs(resids_q11), 
         alternative = "greater",
         method = "spearman", 
         exact = FALSE)

sprintf("Since the p-value is less than 0.05 we reject the Null Hypothesis. There is evidence of Hetereoscedasticity at a 5%% significance level.")

sprintf("There is evidence of Hetereoscedasticity problems in the model")

```

```{r}

#### ANSWER 12 ####

foc = read.csv("FOC.csv")
#foc

sales12 = foc$SALES
time12 = foc$TIME

model_q12 = lm(sales12 ~ time12)
summary(model_q12)

resids_q12 = residuals(model_q12)
fitted_q12 = predict(model_q12)

# Informal Detection by plotting Residuals vs Fitted values

plot(fitted_q12, resids_q12)
sprintf("From the plot we can infer that the condition of homoscedasticity is being violated.")

# Formal Detection via Glejser Test

### H0: B2 = 0 (No heteroscedasticity)
### H1: B2 =/= 0 (there is heteroscedasticity)

glejser_q12 = lm(abs(resids_q12) ~ time12)
summary(glejser_q12)

glejser_q12.2 = lm(abs(resids_q12) ~ sqrt(time12))
summary(glejser_q12.2)

sprintf("Since the p-value is extremely low we can reject the Null Hypothesis. There is significant evidence of Heteroscedasticity.")

# Formal Detection via Park Test

### H0: B2 = 0 (No heteroscedasticity)
### H1: B2 =/= 0 (there is heteroscedasticity)

park_q12 = lm(log(resids_q12^2) ~ log(time12))
summary(park_q12)

sprintf("Since the p-value is extremely low we can reject the Null Hypothesis. There is significant evidence of Heteroscedasticity.")

# Tranforming the Y variable

sales12_trans = log(sales12)

# Informal Detection

model_q12_trans = lm(sales12_trans ~ time12)
summary(model_q12_trans)

resids_q12_trans = residuals(model_q12_trans)
fitted_q12_trans = predict(model_q12_trans)

plot(fitted_q12_trans, resids_q12_trans)

sprintf("By looking at the plot for fitted vs residuals values of the transformed model we can see the variance is now constant.")

# Park Test
### H0: B2 = 0 (No heteroscedasticity)
### H1: B2 =/= 0 (there is heteroscedasticity)

park_q12_trans = lm(log(resids_q12_trans^2) ~ log(time12))
summary(park_q12_trans)

sprintf("We fail to reject the Null Hypothesis at the 5%% significance level. There is no evidence of Heteroscedasticity.")

#Predicting value for Week 300

sprintf("The predicted value for week 300 is %f.",exp(predict(model_q12_trans, data.frame(time12 = c(300)))))

```

```{r}

#### ANSWER 13 ####

woody = read.csv("Woody.csv")
#woody

customers = woody$Y
competition = woody$N
population = woody$P
income = woody$I

#Creating OLS Model
model_q13 = lm(customers ~ competition + population + income)
summary(model_q13)

# Part A
resids_q13 = residuals(model_q13)

# Breusch Pagan test: Regressing residuals square on independent variables used in OLS equation

# Perform an F-test for overall significance to test for heteroscedasticity
# H0: alpha_1 = alpha_2 = alpha_3 = 0
# H1: At least one alpha =/= 0

bptest13 = lm((resids_q13^2) ~ competition + population + income)
summary(bptest13)

sprintf("Since the p-value for the global F-Test is not significant we fail to reject the null hypothesis. There is no evidence of heteroscedasticity in the data.")

# Part B

library("lmtest")
bptest(model_q13)

sprintf("THe result is the same using bptest as in Part A. There is no evidence of Heteroscedasticity.")

# Part C

# White's General Homescedasticity Test

model_white13 = lm((resids_q13^2) ~ competition + population + income + I(competition^2) + I(population^2) + I(income^2) + competition*population + competition*income + population*income + competition*population*income)
summary(model_white13)

chisq_value_13 = summary(model_white13)$r.squared*nrow(woody)
chisq_value_13

pchisq(chisq_value_13,9,lower.tail = FALSE)

sprintf("Since the p-value for the chi-squared statistic is insignificant, we fail to reject the Null Hypothesis. There is no evidence of Heteroscedasticity.")

# Part D
sprintf("The findings from Breusch Pagan Test and White's general test are consistent.")

```

```{r}

#### ANSWER 14 ####

economist = read.csv("EconomistSalary.csv")
economist$Median.salary.... = gsub(',','',economist$Median.salary....)
economist$Median.salary.... = as.integer(economist$Median.salary....)

salary = economist$Median.salary....
age = c(22, 27, 32, 37, 42, 47, 52, 57, 62, 67, 70)

# Part A
model14 = lm(salary ~ age)
summary(model14)

# Part B
# Error variance is proportional to age, applying WLS transformation.

sqrt_age = sqrt(age)
salary_trans = salary/sqrt_age
recip_sqrt_age = 1/sqrt_age

model14b = lm(salary_trans ~ 0 + recip_sqrt_age + sqrt_age)
summary(model14b)

# Part C
recip_age = 1/age
salary_trans_2 = salary/age

model14c = lm(salary_trans_2 ~ recip_age)
summary(model14c)

# Part D

# Plot for B
resids14b = residuals(model14b)
pred14b = predict(model14b)
plot(age,resids14b)

# Plot for C
resids14c = residuals(model14c)
pred14c = predict(model14c)
plot(age,resids14c)

sprintf("In both cases a systematic pattern is detected.")

# Glejser Test for Model B
# H0: beta = 0
# H1: beta != 0
glejser14b1 = lm(abs(resids14b) ~ recip_sqrt_age)
summary(glejser14b1)

# H0: beta = 0
# H1: beta != 0
glejser14b2 = lm(abs(resids14b) ~ sqrt_age)
summary(glejser14b2)

# Park Test for Model C

# H0: beta = 0
# H1: beta != 0
park14c = lm(log(resids14c^2) ~ log(recip_age))
summary(park14c)

sprintf("We are unable to reject the Null Hupothesis in either transofrmation case, therefore we can infer that the transformed equations are homoscedastic. However from visually analysing the plotted residuals a clear pattern is detected which contradicts the results from the heteroscedasticity tests.")

```

```{r}

#### ANSWER 15 ####

ski = read.csv("SkiSales.csv")
#ski

tickets = ski$Tickets
snow = ski$Snowfall
temp = ski$Temperature

# Standard OLS Model

model15 = lm(tickets ~ snow + temp)
resids15 = residuals(model15)
summary(model15)

# Testing for Independence (Autocorrelation)
# Informal
### Graphical - Residuals versus Time
timeperiods <- 0:19
plot(x=timeperiods, type="b", y=resids15, pch=19, 
     xlab = "Time", ylab = "Residuals", 
     main = "Time-Sequence Plot")
abline(h=0)

# Error values show a constant upward trend

### Graphical - Residuals(t) versus Residuals (t-1)
lag.plot(resids15, lags = 1, do.lines = FALSE, 
         diag = FALSE, 
         main = "Residuals versus Lag 1 Residuals")
abline(h=0, v=0)

sprintf("There is evidence of positive autocorrelation in the data.")

# Formal
#Durbin Watson test

# H0: There is no autocorrelation
# H1: There is autocorrelation

lmtest::dwtest(model15)

ts_resids15 <- ts(resids15)
ts_resids15_lag_1 <- stats::lag(ts_resids15, -1)
D <- ts_resids15 - ts_resids15_lag_1
D_sq <- D^2
e_sq <- resids15^2
dw <- sum(D_sq)/sum(e_sq)
dw

sprintf("Since the p-value from the Durbin Watson test is highly significant, we conclude that the data has significant autocorrelation.")

# Testing for Normality
# Informal
hist(tickets)
hist(snow)
hist(temp)
hist(resids15)
# Formal

# H0: Distribution is normal
# H1: Distribution is not normal
shapiro.test(tickets)
shapiro.test(snow)
shapiro.test(temp)
shapiro.test(resids15)

sprintf("The snowfall data is not normally distributed.")

# Testing for Variance

# Informal
plot(predict(model15),resids15)
sprintf("The residuals appear to be randomly distributed.")

# Formal
# Modified White's Test
# H0: beta1 = beta2 = 0
# H1: Atleast one beta != 0
variancecheck = lm((resids15^2) ~ predict(model15) + (predict(model15)^2))
summary(variancecheck)

sprintf("We fail to reject the Null Hypothesis, therefore we can assume constant variance")

sprintf("The data violates assumptions of Independence.")

# New Model
ski$timeperiods = c(0:19)

model15_new = lm(tickets ~ snow + temp + ski$timeperiods)
summary(model15_new)
resids15_new = residuals(model15_new)

# Independence

# H0: There is no autocorrelation
# H1: There is autocorrelation

lmtest::dwtest(model15_new)
sprintf("Since the p-value is large we fail to reject the Null hypothesis.")

# Normality

# H0: Data is Normally distributed
# H1: Data is not Normally Distributed

shapiro.test(resids15_new)
sprintf("Since the p-value is large we fail to reject the Null hypothesis.")

# Constant Variance
# Modified White's Test

variancecheck_new = lm((resids15_new^2) ~ predict(model15_new) + (predict(model15_new)^2))
summary(variancecheck_new)

sprintf("We fail to reject the Null Hypothesis at a significance level of 5%%.")

sprintf("After including Time Periods as an additional explanatory variable in the model all assumptions are satisfied.")

```

```{r}

#### ANSWER 16 ####

compprod = read.csv("CompensationAndProductivity.csv")
#compprod

wages16 = compprod$Y
prod16 = compprod$X

# Part A
model16a = lm(wages16 ~ prod16)
summary(model16a)
resids16a = residuals(model16a)

#Residual vs time plot
timeperiods16 = 0:45
plot(x=timeperiods16, type="b", y=resids16a, pch=19, 
     xlab = "Time", ylab = "Residuals", 
     main = "Time-Sequence Plot")
abline(h=0)

sprintf("The values follow each other closely implying positive auto correlation.")

#DW statistic
ts_resids16a <- ts(resids16a)
ts_resids16a_lag_1 <- stats::lag(ts_resids16a, -1)
Diff <- ts_resids16a - ts_resids16a_lag_1
Diff_sq <- Diff^2
E_sq <- resids16a^2
dwstat <- sum(Diff_sq)/sum(E_sq)
dwstat
sprintf("The durbin watson statistic is extremely close to 0 implying positive correlation")

# H0: There is no autocorrelation
# H1: There is autocorrelation

dwtest(model16a)
sprintf("The p-value is extremely significant, we can infer there is significant evidence of autocorrelation.")

# Adding timeperiods to model

model16a.2 = lm(wages16 ~ prod16 + timeperiods16)
resids16a.2 = residuals(model16a.2)
summary(model16a.2)

plot(x=timeperiods16, type="b", y=resids16a.2, pch=19, 
     xlab = "Time", ylab = "Residuals", 
     main = "Time-Sequence Plot")
abline(h=0)

dwtest(model16a.2)

sprintf("Including the time index does not change anything in the model.")

# Part B
lag_sales16 = lag(wages16,1)
model16b = lm(wages16 ~ prod16 + lag_sales16)
summary(model16b)
resids16b = residuals(model16b)

plot(x=0:44, type="b", y=resids16b, pch=19, 
     xlab = "Time", ylab = "Residuals", 
     main = "Time-Sequence Plot")
abline(h=0)

sprintf("The overall model is highly significant with a high R-squared value as well. Both X variable and Lagged Y variable are highly significant as well. From the residuals vs time series plot we can see that residuals do not follow a single direction for long and display a more random pattern. We can infer that including the lagged Y variable has reduced the autocorrelation significantly.")


# Part C

# Cannot use DW d-test as model includes lagged Y term, have to use DW h-test instead
# Since n>30 we can assume that the sample size is large and therefore h-statistic will be normally distributed

lag_sales16 = lag(wages16,1)
model16b = lm(wages16 ~ prod16 + lag_sales16)
summary(model16b)
resids16b = residuals(model16b)

dwtest(model16b)

rho_hat = 1-1.3765/2
rho_hat
h_stat = rho_hat*sqrt(44/(1-44*(0.05826)^2))
h_stat

sprintf("Since h_stat > 1.96 we can reject the Null Hypothesis and infer that there is still evidence of first order autocrrelation.")


```



```{r}

#### ANSWER 17 ####

diet = read.csv("DietEffect.csv")
#diet
hist(diet$Time) # Data is not distributed normally

# Part 1

# Objective: Compare performances between two populations, dieting and not dieting
# Type of Data: Interval, Non Normal (Data is not normal)
# Types of Samples: Independent 2 samples (Block)
# Definition: Population 1 - dieting, Population 2 - not dieting
# Test to be used: Non Parametric, Wilcoxson Rank Sum Test
# 5% significant

# H0: Population locations are the same
# H1: Population 1 is to left of Population 2

dietpart1 = diet[c("Diet.","Time")]
#dietpart1

wilcox.test(dietpart1$Time ~ dietpart1$Diet., alt="less", paired = FALSE, exact = FALSE, conf.level = 0.95)
sprintf("Since the p-value is large, we fail to reject the Null Hypothesis.")

# Part 2

# Objective: Compare performances between two populations, dieting and not dieting
# Type of Data: Ordinal
# Types of Samples: Independent 2 samples (Block)
# Definition: Population 1 - dieting, Population 2 - not dieting
# Test to be used: Non Parametric, Wilcoxson Rank Sum Test
# 5% significant

# H0: Population locations are the same
# H1: Population 1 is to left of Population 2

dietpart2 = diet[c("Diet.","Letters")]
dietpart2$Diet. = as.factor(dietpart2$Diet.)

wilcox.test(dietpart2$Letters ~ dietpart2$Diet., alt="less", paired = FALSE, exact = FALSE, conf.level = 0.95)
sprintf("Since the p-value is lower than 0.05, we reject the Null Hypothesis.")

# Part 3

# Objective: Compare performances between two populations, dieting and not dieting
# Type of Data: Ordinal
# Types of Samples: Independent 2 samples (Block)
# Definition: Population 1 - dieting, Population 2 - not dieting
# Test to be used: Non Parametric, Wilcoxson Rank Sum Test
# 5% significant

# H0: Population locations are the same
# H1: Population 1 is to left of Population 2

dietpart3 = diet[c("Diet.","Words")]
#dietpart3

wilcox.test(dietpart3$Words ~ dietpart3$Diet., alt="less", paired = FALSE, exact = FALSE, conf.level = 0.95)
sprintf("Since the p-value is lower than 0.10, we reject the Null Hypothesis at a 10%% significance level.")

sprintf("There was no evidence of adverse effects of dieting on solving the mathematical problems. However there is evidence that dieting adversely affects brain memory. ")
```

```{r}

#### ANSWER 18 ####

consumption = read.csv("Consumption.csv")
#consumption

consump = consumption$con
dispincome = consumption$dpi
interests = consumption$aaa
years = consumption$year

# Part A
model18a = lm(consump ~ dispincome + interests)
summary(model18a)

#Part B
resids18a = residuals(model18a)
timeperiod18 = 1:62
plot(timeperiod18, resids18a)

#Part C
# H0: rho = 0
# H1: rho > 0
dwtest(model18a)
sprintf("The p-value is extremely slow, hence we can reject the Null Hypothesis. There is evidence of Positive AutoCorrelation")

#Part D
# H0: rho = 0
# H1: rho > 0
bgtest(model18a,order=1)
sprintf("The p-value is extremely slow, hence we can reject the Null Hypothesis. There is evidence of Positive AutoCorrelation. It agrees with DW d-statistic test")

#Part E
# GLS
model18a = lm(consump ~ dispincome + interests)
summary(model18a)
rho_hat18 = 1-0.38577/2
rho_hat18

#Part G
#install.packages("sandwich")
library("sandwich")

nw18 = NeweyWest(model18a, lag = 1)
coeftest(model18a, vcov = nw18)

#Part H
summary(model18a)

sprintf("The coefficients are the same as the OLS calculations. The p-values for the coefficients are lower for Newey west calculated model as compared to the original model. ")

```

